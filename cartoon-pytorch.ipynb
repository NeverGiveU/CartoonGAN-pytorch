{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### pacakages importing\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    from torch.hub import load_state_dict_from_url\n",
    "except ImportError:\n",
    "    from torch.utils.model_zoo import load_url as load_state_dict_from_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### definition of class Instance-Normalization\n",
    "class InstanceNormalization(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-9):\n",
    "        super(InstanceNormalization, self).__init__()\n",
    "        self.scale = nn.Parameter(torch.FloatTensor(dim))   \n",
    "        self.shift = nn.Parameter(torch.FloatTensor(dim))\n",
    "        # x' = sclae* * x + shift\n",
    "        self.eps = eps\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # initialization or reseting\n",
    "        self.scale.data.uniform_()\n",
    "        self.shift.data.zero_()\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        '''\n",
    "        x --[N, C, H, W]\n",
    "        mean, var, scale_broadcast, shift_broadcast --[N, C, H, W]\n",
    "        '''\n",
    "        n = x.size(2)*x.size(3)\n",
    "        t = x.view(x.size(0), x.size(1), n)\n",
    "        # [N, C, H, W] -> [N, C, HW]\n",
    "        '''unsqueeze(i) where i means we will expand the dim as the i-th dim'''\n",
    "        \n",
    "        mean = torch.mean(t, 2).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
    "        var = torch.var(t, 2).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
    "        # [N, C] -> [H, C, 1] -> [N, C, 1, 1] -> [N, C, H, W]\n",
    "        # the avg is based on each channel crossing the 'H-W' plane\n",
    "        \n",
    "        scale_broadcast = self.scale.unsqueeze(1).unsqueeze(1).unsqueeze(0)\n",
    "        scale_broadcast = scale_broadcast.expand_as(x)\n",
    "        shift_broadcast = self.shift.unsqueeze(1).unsqueeze(1).unsqueeze(0)\n",
    "        shift_broadcast = shift_broadcast.expand_as(x)\n",
    "        # [dim] -> [dim, 1] -> [dim, 1, 1] -> [1, dim, 1, 1] -> [N, dim, H, W]\n",
    "        # for each channel\n",
    "            \n",
    "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        out = out * scale_broadcast + shift_broadcast\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### definition of Generator\n",
    "class CartoonGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CartoonGenerator, self).__init__()\n",
    "        \n",
    "        \"\"\"Down Convolution\"\"\"\n",
    "        self.refpad0_1_1 = nn.ReflectionPad2d(3)\n",
    "        self.conv0_1_1 = nn.Conv2d(3, 64, 7)\n",
    "        self.in0_1_1 = InstanceNormalization(64)\n",
    "        # relu\n",
    "        # [H, W]\n",
    "        \n",
    "        self.conv0_2_1 = nn.Conv2d(64, 128, 3, 2, 1)\n",
    "        self.conv0_2_2 = nn.Conv2d(128, 128, 3, 1, 1)\n",
    "        self.in0_2_1 = InstanceNormalization(128)\n",
    "        # relu\n",
    "        # [H/2, W/2]\n",
    "    \n",
    "        self.conv0_3_1 = nn.Conv2d(128, 256, 3, 2, 1)\n",
    "        self.conv0_3_2 = nn.Conv2d(256, 256, 3, 1, 1)\n",
    "        self.in0_3_1 = InstanceNormalization(256)\n",
    "        # relu\n",
    "        # [H/4, W/4]\n",
    "        \n",
    "        \"\"\"Residual Blocks\"\"\"\n",
    "        self.refpad0_4_1 = nn.ReflectionPad2d(1)\n",
    "        self.conv0_4_1 = nn.Conv2d(256, 256, 3)\n",
    "        self.in0_4_1 = InstanceNormalization(256)\n",
    "        # relu\n",
    "        self.refpad0_4_2 = nn.ReflectionPad2d(1)\n",
    "        self.conv0_4_2 = nn.Conv2d(256, 256, 3)\n",
    "        self.in0_4_2 = InstanceNormalization(256)\n",
    "        # + input\n",
    "        \n",
    "        self.refpad0_5_1 = nn.ReflectionPad2d(1)\n",
    "        self.conv0_5_1 = nn.Conv2d(256, 256, 3)\n",
    "        self.in0_5_1 = InstanceNormalization(256)\n",
    "        # relu\n",
    "        self.refpad0_5_2 = nn.ReflectionPad2d(1)\n",
    "        self.conv0_5_2 = nn.Conv2d(256, 256, 3)\n",
    "        self.in0_5_2 = InstanceNormalization(256)\n",
    "        # + input\n",
    "        \n",
    "        self.refpad0_6_1 = nn.ReflectionPad2d(1)\n",
    "        self.conv0_6_1 = nn.Conv2d(256, 256, 3)\n",
    "        self.in0_6_1 = InstanceNormalization(256)\n",
    "        # relu\n",
    "        self.refpad0_6_2 = nn.ReflectionPad2d(1)\n",
    "        self.conv0_6_2 = nn.Conv2d(256, 256, 3)\n",
    "        self.in0_6_2 = InstanceNormalization(256)\n",
    "        # + input\n",
    "        \n",
    "        self.refpad0_7_1 = nn.ReflectionPad2d(1)\n",
    "        self.conv0_7_1 = nn.Conv2d(256, 256, 3)\n",
    "        self.in0_7_1 = InstanceNormalization(256)\n",
    "        # relu\n",
    "        self.refpad0_7_2 = nn.ReflectionPad2d(1)\n",
    "        self.conv0_7_2 = nn.Conv2d(256, 256, 3)\n",
    "        self.in0_7_2 = InstanceNormalization(256)\n",
    "        # + input\n",
    "        \n",
    "        self.refpad0_8_1 = nn.ReflectionPad2d(1)\n",
    "        self.conv0_8_1 = nn.Conv2d(256, 256, 3)\n",
    "        self.in0_8_1 = InstanceNormalization(256)\n",
    "        # relu\n",
    "        self.refpad0_8_2 = nn.ReflectionPad2d(1)\n",
    "        self.conv0_8_2 = nn.Conv2d(256, 256, 3)\n",
    "        self.in0_8_2 = InstanceNormalization(256)\n",
    "        # + input\n",
    "        \n",
    "        self.refpad0_9_1 = nn.ReflectionPad2d(1)\n",
    "        self.conv0_9_1 = nn.Conv2d(256, 256, 3)\n",
    "        self.in0_9_1 = InstanceNormalization(256)\n",
    "        # relu\n",
    "        self.refpad0_9_2 = nn.ReflectionPad2d(1)\n",
    "        self.conv0_9_2 = nn.Conv2d(256, 256, 3)\n",
    "        self.in0_9_2 = InstanceNormalization(256)\n",
    "        # + input\n",
    "        \n",
    "        self.refpad0_10_1 = nn.ReflectionPad2d(1)\n",
    "        self.conv0_10_1 = nn.Conv2d(256, 256, 3)\n",
    "        self.in0_10_1 = InstanceNormalization(256)\n",
    "        # relu\n",
    "        self.refpad0_10_2 = nn.ReflectionPad2d(1)\n",
    "        self.conv0_10_2 = nn.Conv2d(256, 256, 3)\n",
    "        self.in0_10_2 = InstanceNormalization(256)\n",
    "        # + input\n",
    "        \n",
    "        self.refpad0_11_1 = nn.ReflectionPad2d(1)\n",
    "        self.conv0_11_1 = nn.Conv2d(256, 256, 3)\n",
    "        self.in0_11_1 = InstanceNormalization(256)\n",
    "        # relu\n",
    "        self.refpad0_11_2 = nn.ReflectionPad2d(1)\n",
    "        self.conv0_11_2 = nn.Conv2d(256, 256, 3)\n",
    "        self.in0_11_2 = InstanceNormalization(256)\n",
    "        # + input\n",
    "        \n",
    "        \"\"\"UP Deconvolution\"\"\"\n",
    "        self.deconv0_12_1 = nn.ConvTranspose2d(256, 128, 3, 2, 1, 1)\n",
    "        self.deconv0_12_2 = nn.ConvTranspose2d(128, 128, 3, 1, 1)\n",
    "        self.in0_12_1 = InstanceNormalization(128)\n",
    "        # relu\n",
    "        \n",
    "        self.deconv0_13_1 = nn.ConvTranspose2d(128, 64, 3, 2, 1, 1)\n",
    "        self.deconv0_13_2 = nn.ConvTranspose2d(64, 64, 3, 1, 1)\n",
    "        self.in0_13_1 = InstanceNormalization(64)\n",
    "        # relu\n",
    "        \n",
    "        self.refpad0_14_1 = nn.ReflectionPad2d(3)\n",
    "        self.deconv0_14_1 = nn.Conv2d(64, 3, 7)\n",
    "        # tanh\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.in0_1_1(self.conv0_1_1(self.refpad0_1_1(x))))\n",
    "        y = F.relu(self.in0_2_1(self.conv0_2_2(self.conv0_2_1(y))))\n",
    "        t04 = F.relu(self.in0_3_1(self.conv0_3_2(self.conv0_3_1(y))))\n",
    "        \n",
    "        \"\"\"\"\"\"\n",
    "        y = F.relu(self.in0_4_1(self.conv0_4_1(self.refpad0_4_1(t04))))\n",
    "        t05 = self.in0_4_2(self.conv0_4_2(self.refpad0_4_2(y))) + t04\n",
    "        \n",
    "        y = F.relu(self.in0_5_1(self.conv0_5_1(self.refpad0_5_1(t05))))\n",
    "        t06 = self.in0_5_2(self.conv0_5_2(self.refpad0_5_2(y))) + t05\n",
    "        \n",
    "        y = F.relu(self.in0_6_1(self.conv0_6_1(self.refpad0_6_1(t06))))\n",
    "        t07 = self.in0_6_2(self.conv0_6_2(self.refpad0_6_2(y))) + t06\n",
    "        \n",
    "        y = F.relu(self.in0_7_1(self.conv0_7_1(self.refpad0_7_1(t07))))\n",
    "        t08 = self.in0_7_2(self.conv0_7_2(self.refpad0_7_2(y))) + t07\n",
    "        \n",
    "        y = F.relu(self.in0_8_1(self.conv0_8_1(self.refpad0_8_1(t08))))\n",
    "        t09 = self.in0_8_2(self.conv0_8_2(self.refpad0_8_2(y))) + t08\n",
    "        \n",
    "        y = F.relu(self.in0_9_1(self.conv0_9_1(self.refpad0_9_1(t09))))\n",
    "        t10 = self.in0_9_2(self.conv0_9_2(self.refpad0_9_2(y))) + t09\n",
    "        \n",
    "        y = F.relu(self.in0_10_1(self.conv0_10_1(self.refpad0_10_1(t10))))\n",
    "        t11 = self.in0_10_2(self.conv0_10_2(self.refpad0_10_2(y))) + t10\n",
    "        \n",
    "        y = F.relu(self.in0_11_1(self.conv0_11_1(self.refpad0_11_1(t11))))\n",
    "        y = self.in0_11_2(self.conv0_11_2(self.refpad0_11_2(y))) + t11\n",
    "        \"\"\"\"\"\"\n",
    "        \n",
    "        y = F.relu(self.in0_12_1(self.deconv0_12_2(self.deconv0_12_1(y))))\n",
    "        y = F.relu(self.in0_13_1(self.deconv0_13_2(self.deconv0_13_1(y))))\n",
    "        y = torch.tanh(self.deconv0_14_1(self.refpad0_14_1(y)))\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### definition of Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(3, 32, 3, 1)\n",
    "        # leak_relu\n",
    "        \n",
    "        self.conv_2_1 = nn.Conv2d(32, 64, 3, 2, 1)\n",
    "        # leak_relu\n",
    "        self.conv_2_2 = nn.Conv2d(64, 128, 3, 1, 1)\n",
    "        self.in_2 = InstanceNormalization(128)\n",
    "        # leak_relu\n",
    "        \n",
    "        self.conv_3_1 = nn.Conv2d(128, 128, 3, 2, 1)\n",
    "        # leak_relu\n",
    "        self.conv_3_2 = nn.Conv2d(128, 256, 3, 1, 1)\n",
    "        self.in_3 = InstanceNormalization(256)\n",
    "        # leak_relu\n",
    "        \n",
    "        self.conv_4 = nn.Conv2d(256, 256, 3, 1, 1)\n",
    "        self.in_4 = InstanceNormalization(256)\n",
    "        # leak_relu\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 1, 3, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv_1(x), negative_slope=0.2)\n",
    "        \n",
    "        x = F.leaky_relu(self.conv_2_1(x), negative_slope=0.2)\n",
    "        x = F.leaky_relu(self.in_2(self.conv_2_2(x)), negative_slope=0.2)\n",
    "        \n",
    "        x = F.leaky_relu(self.conv_3_1(x), negative_slope=0.2)\n",
    "        x = F.leaky_relu(self.in_3(self.conv_3_2(x)), negative_slope=0.2)\n",
    "        \n",
    "        x = F.leaky_relu(self.in_4(self.conv_4(x)), negative_slope=0.2)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### definition of losses\n",
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.partial_loss = nn.L1Loss()\n",
    "        \n",
    "    def __call__(self, F1, F2):\n",
    "        loss = 0\n",
    "        L1 = len(F1)\n",
    "        L2 = len(F2)\n",
    "        if L1 != L2:\n",
    "            raise Exception(\"Unmatch input features\")\n",
    "        for i in range(L1):\n",
    "            loss += self.partial_loss(F1[i], F2[i])\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "class AdversialLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdversialLoss, self).__init__()\n",
    "        self.register_buffer('true_label', torch.tensor(1.0))\n",
    "        self.register_buffer('false_label', torch.tensor(0.0))\n",
    "        \n",
    "        self.loss = nn.MSELoss()\n",
    "    \n",
    "    def get_target_tensor(self, prediction, target_is_real):\n",
    "        if target_is_real is True:\n",
    "            target_tensor = self.true_label\n",
    "        else:\n",
    "            target_tensor = self.false_label\n",
    "        return target_tensor.expand_as(prediction)\n",
    "    \n",
    "    def __call__(self, prediction, target_is_real):\n",
    "        target_tensor = self.get_target_tensor(prediction, target_is_real)\n",
    "        loss = self.loss(prediction, target_tensor)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### self-implementation of VGG16_bn for feature encoding\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "    \n",
    "        self.conv1_1 = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(inplace=True)\n",
    "                )\n",
    "        self.conv1_2 = nn.Sequential(\n",
    "                nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(inplace=True)\n",
    "                )\n",
    "        self.conv2_1 = nn.Sequential(\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(inplace=True)\n",
    "                )\n",
    "        self.conv2_2 = nn.Sequential(\n",
    "                nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(inplace=True)\n",
    "                )\n",
    "        self.conv3_1 = nn.Sequential(\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(inplace=True)\n",
    "                )\n",
    "        self.conv3_2 = nn.Sequential(\n",
    "                nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(inplace=True)\n",
    "                )\n",
    "        self.conv3_3 = nn.Sequential(\n",
    "                nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(inplace=True)\n",
    "                )\n",
    "        self.conv4_1 = nn.Sequential(\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU(inplace=True)\n",
    "                )\n",
    "        self.conv4_2 = nn.Sequential(\n",
    "                nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU(inplace=True)\n",
    "                )\n",
    "        self.conv4_3 = nn.Sequential(\n",
    "                nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU(inplace=True)\n",
    "                )\n",
    "        self.conv5_1 = nn.Sequential(\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU(inplace=True)\n",
    "                )\n",
    "        self.conv5_2 = nn.Sequential(\n",
    "                nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU(inplace=True)\n",
    "                )\n",
    "        self.conv5_3 = nn.Sequential(\n",
    "                nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y11 = self.conv1_1(x)\n",
    "        y12 = self.conv1_2(y11)\n",
    "        \n",
    "        y21 = self.conv2_1(y12)\n",
    "        y22 = self.conv2_2(y21)\n",
    "        \n",
    "        y31 = self.conv3_1(y22)\n",
    "        y32 = self.conv3_2(y31)\n",
    "        y33 = self.conv3_3(y32)\n",
    "        \n",
    "        y41 = self.conv4_1(y33)\n",
    "        y42 = self.conv4_2(y41)\n",
    "        y43 = self.conv4_3(y42)\n",
    "        \n",
    "        y51 = self.conv5_1(y43)\n",
    "        y52 = self.conv5_2(y51)\n",
    "        y53 = self.conv5_3(y52)\n",
    "        \n",
    "        # return y12, y22, y33, y43, y53\n",
    "        return y11, y12, y21, y22, y31, y32, y33, y41, y42, y43, y51, y52, y53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### parameters configuration\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lr', type=int, default=1e-4, help='learning rate, default=0.0001')\n",
    "parser.add_argument('--batch_size', type=int, default=4, help='batch size during training, default=8')\n",
    "\n",
    "opt = parser.parse_args()\n",
    "print(opt)\n",
    "\n",
    "IMG_SIZE = 256 # make sure that, the data you prepared satisfies that H == W >= IMG_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### two metrics for classification\n",
    "def binary_cross_entropy_metric(y_pred, y_true):\n",
    "    eps = 1e-12\n",
    "    y_pred = np.clip(y_pred, eps, 1.0-eps)\n",
    "    return -(y_true * np.log(y_pred+eps) + (1-y_true) * np.log(1-y_pred+eps)).mean()\n",
    "\n",
    "def binary_accuracy_metric(y_pred, y_true):\n",
    "    y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "    y_pred = y_pred.astype(np.int32)\n",
    "    return (y_true == y_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### dirs and files for training recording\n",
    "check_pth = os.path.join(os.getcwd(), 'checkpoints')\n",
    "if os.path.exists(check_pth) is not True:\n",
    "    os.mkdir(check_pth)\n",
    "    \n",
    "## the training loss will be recorded\n",
    "txt_pth = os.path.join(check_pth, 'records.txt')                     \n",
    "txt_handle = open(txt_pth, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### data preparing\n",
    "\n",
    "## make sure that, you have training data in \"./datasets\", the structure of which is as like as follows\n",
    "\"\"\"\n",
    "-- datasets\n",
    "  |-- real \n",
    "  |-- comic\n",
    "\n",
    "where, data of photorealistic images are in subdir \"./datasets/real\", in form of ['0.jpg', '1.jpg', '2.jpg', ...]\n",
    "       data of cartoon-like images are in subdir  \"./datasets/comic\", in form of ['0.jpg', '1.jpg', '2.jpg', ...]\n",
    "\"\"\"\n",
    "\n",
    "## now we will try to generate the third kind of data \"./datasets/comic_blurred\"\n",
    "if os.path.exists(os.path.join(os.getcwd(), 'datasets', 'comic_blurred')) is not True:\n",
    "    os.mkdir(os.path.join(os.getcwd(), 'datasets', 'comic_blurred'))\n",
    "    \n",
    "'''Data paths'''\n",
    "datasets_pth = {\n",
    "    'trainA': os.path.join(os.getcwd(), 'datasets', 'real'),\n",
    "    'trainB': os.path.join(os.getcwd(), 'datasets', 'comic'),\n",
    "    'trainC': os.path.join(os.getcwd(), 'datasets', 'comic_blurred')\n",
    "}\n",
    "\n",
    "# the following blurring function are from https://github.com/nijuyr/comixGAN\n",
    "\n",
    "def smooth_image_edges(img, plot=False):\n",
    "    # Get edges\n",
    "    edges = cv2.Canny(img,30,60)\n",
    "\n",
    "    # Dilate edges with kernel (5,5) with 15 iterations\n",
    "    dilated_edges = cv2.dilate(edges,(7,7), iterations=25)\n",
    "    \n",
    "    dilated_edges_to_compare = dilated_edges.copy()\n",
    "    dilated_edges_to_compare[dilated_edges == 0] = -1\n",
    "\n",
    "    # Copy image twice\n",
    "    img_no_dilated_edges, img_only_dilated_edges = img.copy(), img.copy()\n",
    "\n",
    "    # Prepare images with region of only edges and no edges\n",
    "    img_no_dilated_edges[dilated_edges_to_compare != -1] = 0\n",
    "    img_only_dilated_edges[dilated_edges_to_compare == -1] = 0\n",
    "    \n",
    "    # Gaussian blur of the image with region of only edges\n",
    "    blurred_edges = cv2.GaussianBlur(img_only_dilated_edges,(9,9),0)\n",
    "    \n",
    "    # Clip to take only region of edges (without values blurred on the remaining parts of the image)\n",
    "    blurred_edges[dilated_edges_to_compare == -1] = 0\n",
    "\n",
    "    # Final Gaussian blur of sum of images with and without edges\n",
    "    result = blurred_edges + img_no_dilated_edges\n",
    "    result = cv2.GaussianBlur(result,(9,9),0)\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(16,10))\n",
    "        plt.subplot(221),plt.imshow(img[:,:,[2,1,0]])\n",
    "        plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "        plt.subplot(222),plt.imshow(edges, cmap = 'gray')\n",
    "        plt.title('Edges'), plt.xticks([]), plt.yticks([])\n",
    "        plt.subplot(224),plt.imshow(dilated_edges, cmap = 'gray')\n",
    "        plt.title('Dilated edges'), plt.xticks([]), plt.yticks([])\n",
    "        plt.subplot(223),plt.imshow(result[:,:,[2,1,0]])\n",
    "        plt.title('Blurred Image'), plt.xticks([]), plt.yticks([])\n",
    "        plt.show()        \n",
    "    return result\n",
    "\n",
    "# try an example\n",
    "samples = os.listdir(datasets_pth['trainB'])\n",
    "random.shuffle(samples)\n",
    "\n",
    "sample = cv2.imread(os.path.join(datasets_pth['trainB'], samples[0]))\n",
    "result = smooth_image_edges(sample, plot=True)\n",
    "\n",
    "# convert comic data in \"./datasets/comic\" to blurred data in \"./datasets/comic_blurred\"\n",
    "#### to generate the blurred comic image\n",
    "imgs = os.listdir(datasets_pth['trainB'])\n",
    "for img_name in tqdm(imgs):\n",
    "    image = cv2.imread(os.path.join(datasets_pth['trainB'], img_name))\n",
    "    result = smooth_image_edges(image, plot=False)\n",
    "    cv2.imwrite(os.path.join(datasets_pth['trainC'], img_name), result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Global Variables\n",
    "EPOCH = 101\n",
    "gpu = 0\n",
    "omega = 10  # loss = lossAdv + omega * lossCon\n",
    "real_label = 1.0\n",
    "fake_label = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### preparing data\n",
    "trainA_files = os.listdir(datasets_pth['trainA'])\n",
    "trainB_files = os.listdir(datasets_pth['trainB'])\n",
    "trainC_files = os.listdir(datasets_pth['trainC'])\n",
    "l_A = len(trainA_files)\n",
    "l_B = len(trainB_files)\n",
    "l_C = len(trainC_files)\n",
    "l_L = max([l_A, l_B, l_C])\n",
    "    \n",
    "for i in range(l_A):\n",
    "    trainA_files[i] = os.path.join(datasets_pth['trainA'], trainA_files[i])\n",
    "for i in range(l_B):\n",
    "    trainB_files[i] = os.path.join(datasets_pth['trainB'], trainB_files[i])\n",
    "for i in range(l_C):\n",
    "    trainC_files[i] = os.path.join(datasets_pth['trainC'], trainC_files[i])\n",
    "print(\"Finish loading the datasets, and there are: <<{}>> human-faces, and <<{}>> manga-faces!\".format(l_A, l_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### initialize models\n",
    "G = CartoonGenerator()\n",
    "D = Discriminator()\n",
    "    \n",
    "#### loading in pretrained vgg16\n",
    "vgg = VGG()\n",
    "vgg.load_state_dict(torch.load('vgg16.pth'))\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "print(\"Finish initializing the models, and they are: Cartoon-Generator, Cartoon-Discriminator, and VGG19\")\n",
    "\n",
    "\n",
    "'''Loss functions'''\n",
    "criterionCon = ContentLoss()\n",
    "criterionDis = nn.MSELoss()\n",
    "criterionAdv = AdversialLoss()\n",
    "\n",
    "'''Optimizers'''\n",
    "optim_G = optim.Adam(G.parameters(), lr=opt.lr, betas=(0.5, 0.999))\n",
    "optim_D = optim.Adam(D.parameters(), lr=opt.lr, betas=(0.5, 0.999))\n",
    "\n",
    "'''Use GPU'''\n",
    "if gpu >= 0:\n",
    "    G = G.to(gpu)\n",
    "    D = D.to(gpu)\n",
    "    vgg = vgg.to(gpu)\n",
    "\n",
    "    criterionCon = criterionCon.to(gpu)\n",
    "    criterionAdv = criterionAdv.to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### pretrain the generator\n",
    "for epoch in range(50):\n",
    "    '''initialization phase'''\n",
    "    shuffle(trainA_files)\n",
    "    shuffle(trainB_files)\n",
    "    shuffle(trainC_files)\n",
    "        \n",
    "    for i_l in tqdm(range(0, l_L, opt.batch_size)):\n",
    "        X = np.zeros((opt.batch_size, 3, IMG_SIZE, IMG_SIZE))\n",
    "        for c in range(opt.batch_size):\n",
    "            img = Image.open(trainA_files[(i_l * opt.batch_size+c) % l_A])\n",
    "            img.thumbnail((IMG_SIZE, IMG_SIZE))\n",
    "            X[c, :, :, :] = np.array(img).transpose(2,0,1)\n",
    "            \n",
    "        X = torch.from_numpy(X.astype(np.float32))\n",
    "        X = 2*(X/255)-1\n",
    "        # X = Variable(X)\n",
    "                \n",
    "        if gpu >= 0:\n",
    "            X = X.to(gpu)\n",
    "                \n",
    "        Out = G(X)\n",
    "        # print(out.data.size())\n",
    "                \n",
    "        F1 = vgg(Out)\n",
    "        F2 = vgg(X)\n",
    "        loss = criterionCon(F1, F2)\n",
    "                \n",
    "        optim_G.zero_grad()\n",
    "        loss.backward()\n",
    "        optim_G.step()\n",
    "            \n",
    "        log = \"This is the end of the {}-th epoch, the content loss for initialization is: {}.\".format(epoch, loss)\n",
    "        # print(log)\n",
    "        if i_l % 10 == 0:\n",
    "            txt_handle.write(log+'\\n')\n",
    "                \n",
    "        \n",
    "    # save image\n",
    "    if gpu >= 0:\n",
    "        X = X.cpu()\n",
    "        Out = Out.cpu()\n",
    "    x = np.array(X.data[0,:,:,:]).transpose(1,2,0)\n",
    "    o = np.array(Out.data[0,:,:,:]).transpose(1,2,0)\n",
    "            \n",
    "    img_x = Image.fromarray(((x*0.5+0.5)*255).astype(np.uint8))\n",
    "    img_o = Image.fromarray(((o*0.5+0.5)*255).astype(np.uint8))\n",
    "            \n",
    "    img_x.save(os.path.join(os.getcwd(), check_pth, 'x-{}.jpg'.format(epoch)))\n",
    "    img_o.save(os.path.join(os.getcwd(), check_pth, 'o-{}.jpg'.format(epoch)))\n",
    "\n",
    "## save the models\n",
    "if gpu >= 0:\n",
    "    G = G.cpu()\n",
    "torch.save(G.state_dict(), \"./checkpoints/G_pretrained.pth\")\n",
    "if gpu >= 0:\n",
    "    G = G.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### pretrain the discriminator\n",
    "PRETRAIN_DISCRIMINATOR_BATCH_SIZE = 16\n",
    "opt.batch_size = PRETRAIN_DISCRIMINATOR_BATCH_SIZE\n",
    "n_A = int(PRETRAIN_DISCRIMINATOR_BATCH_SIZE / 4)\n",
    "n_B = int(PRETRAIN_DISCRIMINATOR_BATCH_SIZE / 2)\n",
    "n_C = int(PRETRAIN_DISCRIMINATOR_BATCH_SIZE / 4)\n",
    "max_iterations = max(l_A // n_A, l_B // n_B, l_C // n_C)\n",
    "    \n",
    "for epoch in range(50):\n",
    "    shuffle(trainA_files)\n",
    "    shuffle(trainB_files)\n",
    "    shuffle(trainC_files)\n",
    "        \n",
    "    i_A, i_B, i_C = 0, 0, 0\n",
    "    ## get input images as well as corresponding labels\n",
    "    for i in tqdm(range(max_iterations)):\n",
    "        inputs = []\n",
    "        labels = []\n",
    "        n_a, n_b, n_c = 0, 0, 0\n",
    "        # get real\n",
    "        while n_a < n_A:\n",
    "            img = Image.open(trainA_files[i_A % l_A])\n",
    "            i_A += 1\n",
    "            H, W = img.size[0], img.size[1]\n",
    "            if H<IMG_SIZE or W<IMG_SIZE:\n",
    "                continue\n",
    "            img.thumbnail((IMG_SIZE, IMG_SIZE))\n",
    "            #print(img.size)\n",
    "            arr = np.array(img).transpose(2,0,1)\n",
    "            arr = arr/255\n",
    "            arr = torch.from_numpy(arr.astype(np.float32))\n",
    "                \n",
    "            label = np.zeros((1, IMG_SIZE//4, IMG_SIZE//4))\n",
    "            label = torch.from_numpy(label.astype(np.float32))\n",
    "                \n",
    "            inputs.append(arr.unsqueeze(0))\n",
    "            labels.append((label).unsqueeze(0))\n",
    "            n_a += 1\n",
    "                \n",
    "        # get comic\n",
    "        while n_b < n_B:\n",
    "            img = Image.open(trainB_files[i_B % l_B])\n",
    "            i_B += 1\n",
    "            H, W = img.size[0], img.size[1]\n",
    "            if H<IMG_SIZE or W<IMG_SIZE:\n",
    "                continue\n",
    "            img.thumbnail((IMG_SIZE, IMG_SIZE))\n",
    "            #print(img.size)\n",
    "            arr = np.array(img).transpose(2,0,1)\n",
    "            arr = arr/255\n",
    "            arr = torch.from_numpy(arr.astype(np.float32))\n",
    "                \n",
    "            label = np.ones((1, IMG_SIZE//4, IMG_SIZE//4))\n",
    "            label = torch.from_numpy(label.astype(np.float32))\n",
    "                \n",
    "            inputs.append(arr.unsqueeze(0))\n",
    "            labels.append((label).unsqueeze(0))\n",
    "            n_b += 1\n",
    "            \n",
    "        # get comic_blurred\n",
    "        while n_c < n_C:\n",
    "            img = Image.open(trainC_files[i_C % l_C])\n",
    "            i_C += 1\n",
    "            H, W = img.size[0], img.size[1]\n",
    "            if H<IMG_SIZE or W<IMG_SIZE:\n",
    "                continue\n",
    "            img.thumbnail((IMG_SIZE, IMG_SIZE))\n",
    "            #print(img.size)\n",
    "            arr = np.array(img).transpose(2,0,1)\n",
    "            arr = arr/255\n",
    "            arr = torch.from_numpy(arr.astype(np.float32))\n",
    "                \n",
    "            label = np.zeros((1, IMG_SIZE//4, IMG_SIZE//4))\n",
    "            label = torch.from_numpy(label.astype(np.float32))\n",
    "                \n",
    "            inputs.append(arr.unsqueeze(0))\n",
    "            labels.append((label).unsqueeze(0))\n",
    "            n_c += 1\n",
    "            \n",
    "            \n",
    "        ## concatenate\n",
    "        inputs = torch.cat(inputs, 0)\n",
    "        labels = torch.cat(labels, 0)\n",
    "            \n",
    "        ## randomize the order\n",
    "        randomize = np.arange(PRETRAIN_DISCRIMINATOR_BATCH_SIZE)\n",
    "        np.random.shuffle(randomize)\n",
    "        inputs = inputs[randomize]\n",
    "        labels = labels[randomize]\n",
    "\n",
    "        if gpu >= 0:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        optim_D.zero_grad()\n",
    "        preds = D(inputs)\n",
    "\n",
    "        preds[preds < 0.0] = 0.0\n",
    "        preds[preds > 1.0] = 1.0\n",
    "\n",
    "        # print(preds.size(), labels.size())\n",
    "        loss = criterionDis(preds, labels)\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optim_D.step()\n",
    "            \n",
    "        log = (\"The loss is: {}\".format(loss))\n",
    "        if i % 10 == 0:\n",
    "            txt_handle.write(log+'\\n')\n",
    "            \n",
    "    ## validation\n",
    "    print(log)\n",
    "    if gpu >= 0:\n",
    "        preds = preds.cpu()\n",
    "        labels = labels.cpu()\n",
    "        \n",
    "    acc = binary_accuracy_metric(np.array(preds.data), np.array(labels.data))\n",
    "    log = (\"The loss is: {}, The accuracy is: {}\".format(loss, acc))\n",
    "    print(log)\n",
    "    \n",
    "## save the models\n",
    "if gpu >= 0:\n",
    "    D = D.cpu()\n",
    "torch.save(D.state_dict(), \"./checkpoints/D_pretrained.pth\")\n",
    "if gpu >= 0:\n",
    "    D = D.cuda()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### deversial training the entire framework\n",
    "opt.batch_size = 4\n",
    "max_iterations = max(l_A // opt.batch_size, l_B // opt.batch_size, l_C // opt.batch_size)\n",
    "    \n",
    "for epoch in range(EPOCH):\n",
    "    shuffle(trainA_files)\n",
    "    shuffle(trainB_files)\n",
    "    shuffle(trainC_files)\n",
    "    '''training'''\n",
    "    i_A = i_B = i_C = 0\n",
    "    ii = 0\n",
    "    for i_L in tqdm(range(0, max_iterations, opt.batch_size)):\n",
    "        ii += 1\n",
    "            \n",
    "        ## inputs of G\n",
    "        X = np.zeros((opt.batch_size, 3, IMG_SIZE, IMG_SIZE))\n",
    "        c = 0\n",
    "            \n",
    "        while c < opt.batch_size:\n",
    "            img = Image.open(trainA_files[i_A % l_A])\n",
    "            i_A += 1\n",
    "            H, W = img.size[0], img.size[1]\n",
    "            if H < IMG_SIZE or W < IMG_SIZE:\n",
    "                continue\n",
    "            img.thumbnail((IMG_SIZE, IMG_SIZE))\n",
    "            X[c, :, :, :] = np.array(img).transpose(2,0,1)\n",
    "            c += 1\n",
    "        X = X / 255\n",
    "        X = torch.from_numpy(X.astype(np.float32))\n",
    "        if gpu >= 0:\n",
    "            X = X.to(gpu)\n",
    "                \n",
    "        if ii % 3 == 0:\n",
    "            ## train D\n",
    "            loss_D = 0\n",
    "            optim_D.zero_grad()\n",
    "            # 1A: train D on real\n",
    "            # >> prepare the inouts\n",
    "            real_X = np.zeros((opt.batch_size, 3, IMG_SIZE, IMG_SIZE))\n",
    "            c = 0\n",
    "            while c < opt.batch_size:\n",
    "                img = Image.open(trainB_files[i_B % l_B])\n",
    "                i_B += 1\n",
    "                H, W = img.size[0], img.size[1]\n",
    "                if H < IMG_SIZE or W < IMG_SIZE:\n",
    "                    continue\n",
    "                img.thumbnail((IMG_SIZE, IMG_SIZE))\n",
    "                real_X[c, :, :, :] = np.array(img).transpose(2,0,1)\n",
    "                c += 1\n",
    "            real_X = real_X / 255\n",
    "            real_X = torch.from_numpy(real_X.astype(np.float32))\n",
    "            if gpu >= 0:\n",
    "                real_X = real_X.to(gpu)\n",
    "            # >> forward\n",
    "            real_decision = D(real_X)\n",
    "            d_real_error = criterionAdv(real_decision, True)   # torch.ones([opt.batch_size, 3, 256, 256]))\n",
    "            # d_real_error.backward()\n",
    "            loss_D = d_real_error\n",
    "                \n",
    "            # 1B: train D on blur   \n",
    "            # >> prepare the inputs \n",
    "            blur_X = np.zeros((opt.batch_size, 3, IMG_SIZE, IMG_SIZE))\n",
    "            c = 0\n",
    "            # for c in range(opt.batch_size):\n",
    "            while c < opt.batch_size:\n",
    "                img = Image.open(trainC_files[i_C % l_C])\n",
    "                i_C += 1\n",
    "                H, W = img.size[0], img.size[1]\n",
    "                if H < IMG_SIZE or W < IMG_SIZE:\n",
    "                    continue\n",
    "                img.thumbnail((IMG_SIZE, IMG_SIZE))\n",
    "                blur_X[c, :, :, :] = np.array(img).transpose(2,0,1)\n",
    "                c += 1\n",
    "            blur_X = blur_X / 255\n",
    "            blur_X = torch.from_numpy(blur_X.astype(np.float32))\n",
    "            if gpu >= 0:\n",
    "                blur_X = blur_X.to(gpu)\n",
    "            # >> forward\n",
    "            blur_decision = D(blur_X)\n",
    "            d_blur_error = criterionAdv(blur_decision, False)  # torch.zeros([opt.batch_size, 3, 256, 256]))\n",
    "            # d_blur_error.backward()\n",
    "            loss_D += d_blur_error\n",
    "                \n",
    "            # 1C: train D on fake\n",
    "            fake_X = G(X)\n",
    "            d_fake_decision = D(fake_X)\n",
    "            d_fake_error = criterionAdv(d_fake_decision, False)# torch.zeros([opt.batch_size, 3, 256, 256]))\n",
    "            # d_fake_error.backward()\n",
    "            loss_D += d_fake_error\n",
    "                \n",
    "            # 1D: update\n",
    "            loss_D = loss_D / 3\n",
    "            loss_D.backward(retain_graph=True)\n",
    "            optim_D.step()\n",
    "            \n",
    "            \n",
    "        ## 2: train G\n",
    "        loss_G = 0\n",
    "        optim_G.zero_grad()\n",
    "        Out = G(X)\n",
    "        F1 = vgg(Out)\n",
    "        F2 = vgg(X)\n",
    "        loss_con = criterionCon(F1, F2)# x10\n",
    "            \n",
    "            \n",
    "        g_fake_decision= D(Out)\n",
    "        loss_adv = criterionAdv(g_fake_decision, True)\n",
    "            \n",
    "        loss_G = loss_adv + loss_con * 0.5\n",
    "        loss_G.backward()\n",
    "        optim_G.step()\n",
    "            \n",
    "        if i_L % 100 == 0:\n",
    "            log = \"Epoch-{}, iteration-{} >> Content loss of G: {}, Adversarial loss of G: {}, Adversarial loss of D: {}\".format(epoch, i_L, loss_con, loss_adv, loss_D)\n",
    "            txt_handle.write(log+'\\n')\n",
    "            print(log)\n",
    "                \n",
    "    ## save image\n",
    "    if gpu >= 0:\n",
    "        X = X.cpu()\n",
    "        Out = Out.cpu()\n",
    "\n",
    "    x = (np.array(X.data[0,:,:,:])*255).astype(np.uint8).transpose(1,2,0)\n",
    "    o = (np.array(Out.data[0,:,:,:])*255).astype(np.uint8).transpose(1,2,0)\n",
    "        \n",
    "    plt.subplot(121)\n",
    "    plt.title(\"Real image\")\n",
    "    plt.imshow(x)\n",
    "        \n",
    "    plt.subplot(122)\n",
    "    plt.title(\"Cartoonalized image\")\n",
    "    plt.imshow(o)\n",
    "    \n",
    "    plt.savefig(os.path.join(os.getcwd(), check_pth, '%04d-%06d.jpg'%(epoch, i_L)))\n",
    "            \n",
    "## save models\n",
    "if gpu >= 0:\n",
    "    G = G.cpu()\n",
    "torch.save(G.state_dict(), \"./checkpoints/G_adv.pth\")\n",
    "if gpu >= 0:\n",
    "    D = D.cpu()\n",
    "torch.save(D.state_dict(), \"./checkpoints/D_adv.pth\")\n",
    "        \n",
    "txt_handle.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
